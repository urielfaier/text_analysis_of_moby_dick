---
title: "Moby-Dick text analysis using R"
author: "Uriel Faier"
date: "8/4/2022"
output: html_document
---

```{r, include=FALSE}
library(tidyverse) # This includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext) 
library(rvest)
```

## Analysis of textual data and the `Wordle` game 


### Questions: 

#### PART 1 - MOBY-DICK

1.a. Load the complete `Moby dick`  book from the [Gutenberg project](https://www.gutenberg.org) into `R`. The book is available [here](https://www.gutenberg.org/files/2701/2701-h/2701-h.htm).
Extract the text from the html as a long string, and print the first line of the text in the file (starting with `The Project Gutenberg ...`)

b. Split the text string into words, separated by spaces, commas (`,`), periods (`.`), and new line characters (`\n` and `\r`). How many words are there? 
Compute the distribution of lengths of words you got, and plot using a bar-plot. What are the `median`, `mean`, `longest` and `most common` word lengths? <br>
**Note:** some of the "words" you will get will still contain non-English characters (e.g. numbers, `-`, `;` or other characters). Don't worry about it. We will parse the words further later when needed. 
c. Count the words frequencies in the text - i.e. the number of times each unique word appears in the text.
Show the top 10 most frequent words with their frequencies. Is the list of top words surprising? explain. 


2.a. Split the book text into `chapters`, such that you obtain an array of strings, one per chapter. 
Count and plot the number of words per each chapter (y-axis) vs. the chapter index (1,2,3.., on x-axis). 
(each chapter is splitted to word in the same manner as in Qu. 1). <br>
**Hint:** Chapters begin with the string `CHAPTER` followed by a space and then the chapter's number and a period. For example: `CHAPTER 2.` is at the start of chapter 2. But beware - this pattern by itself is not enough to identify correctly all chapters starts and end. You will need to *look at the text* in order to decide what patterns to look for when splitting the data into chapters. 

b. Write a function that receives as input a query word, and an array of strings representing the chapters. The function returns a vector of the `relative frequencies` of the word in each chapter. That is, if for example the word `time` appears six times in the first chapter, and there are overall 
$3247$ words in this chapter, then the first entry in the output vector of the function will be $6/3247 \approx 0.0018$. <br>
Apply the function to the following words `Ahab`, `Moby`, `sea`. Plot for each one of them the trend, i.e. the frequency vs. chapter, with chapters in increasing orders. Do you see a different behavior for the different words? in which parts of the book are they frequent? 


3.a. Suppose that Alice and Bob each choose independently and uniformly at random a single word from the book. That is, each of them chooses a random word instance from the book, taking into account that words have different frequencies (i.e. if for example the word `the` appears $1000$ times, and the word `Ishmael` appears only once, then it is $1000$-times more likely to choose the word `the` because each of its instances can be chosen). What is the probability that they will pick the same word? 
Answer in two ways: <br>
(i) Derive an exact formula for this probability as a function of the words relative frequencies, and compute the resulting value for the word frequencies you got for the book. <br>
(ii) Simulate $B=100,000$ times the choice of Alice and Bob and use these simulations to estimate the probability that they chose the same word. <br>
Explain your calculations in both ways and compare the results. Are they close to each other? 

b. Suppose that instead, we took all **unique** words that appear in the book, and then Alice and Bob would choose each independently and uniformly at random a single word from the list of unique words. What would be the probability that they chose the same word in this case? is it lower, the same, or higher then the probability in (a.)? explain why. 


4.a. Extract from the book a list of all `five-letter` words. Keep only words that have only English letters. Convert all to lower-case. How many words are you left with? how many unique words? 
Show the top 10 most frequent five-letter words with their frequencies.  

b. Compute letter frequencies statistics of the five-letter words: 
That is, for each of the five locations in the word (first, second,..), how many times each of the English letters `a`, `b`,...,`z` appeared in your (non-unique) list of words. Store the result in a `26-by-5` table and show it as a heatmap. Which letter is most common in each location? Do you see a strong effect for the location? 


c. Consider the following random model for typing words: we have a `26-by-5` table of probabilities $p_{ij}$ for i from $1$ to $5$, 
and $j$ going over all $26$ possible English letters (assuming lower-case). (This table stores the parameters of the model).
Here, $p_{ij}$ is the probability that the $i$-th letter in the word will be the character $j$. 
Now, each letter $i$ is typed from a categorical distribution over the $26$ letters, with probability $p_{ij}$ of being the character $j$, and the letters are drawn independently for different values of $i$. 
For example,  using $p_{5s}=0.3$ we will draw words such that the probability of the last character being `s` will be $0.3$. <br>
For each five-letter word $w$, its likelihood under this model is defined simply as the probability of observing this word when drawing a word according to this model, that is, if $w=(w_1,w_2,w_3,w_4,w_5)$ with $w_i$ denoting the $i$-th letter, then $Like(w ; p) = \prod_{i=1}^5 p_{i w_i}$. <br>
Write a function that receives a `26-by-5` table of probabilities and an array of words (strings), and computes the likelihood of each word according to this model. <br>
Run the function to compute the likelihood of all unique five-letter words from the book, with the frequency table you computed in 4.b. normalized to probabilities. Show the top-10 words with the highest likelihood. 



#### PART 2 - WORDLE

In `wordle`, the goal is to guess an unknown five-letter English word. At each turn, we guess a word, and get the following feedback: the locations at which our guess matches the unknown word (`correct`), the locations at which our guess has a letter that appears in the unknown word but in a different location (`wrong`), and the locations at which our guess contains a letter that is not present in the unknown word (`miss`).

We supply to you a function called `wordle_match`, that receives as input a guess word and the true word (two strings), and returns an array of the same length indicating if there was a `correct` match (1), a match in the `wrong` location (-1), or a `miss` (0). For example: calling `match_words("honey", "bunny")` will yield the array: `[0, 0, 1, 0, 1]`, whereas calling `match_words("maple", "syrup")` will yield the array `[0, 0, -1, 0, 0]`. 

**Note:** It is allowed for both the unknown word and the guess word to contain the same letter twice or more. In that case, we treat each letter in the guess as a `wrong` match if the same letter appears elsewhere in the unknown word. This is a bit different from the rules of the `wordle` game and is used for simplification here. 


5.a. Download the list of five-letter words from [here](https://www-cs-faculty.stanford.edu/~knuth/sgb-words.txt). This list contains most common English five-letter words (each word appears once).  
Compute and display the `26-by-5` table of frequencies for this word list, in similar to Qu. 4.b.
Do you see major differences between the two tables? why? 

b. Write a function that receives an array of guess words, an array of their corresponding matches to the unknown word (i.e. a two-dimensional array), and a `disctionary` - i.e. an array of legal English words. 
The function should return all the words in the dictionary that are consistent with the results of the previous guesses. For example, if we guessed `maple` and our match was the array `[1, 0, -1, 0, 0]`, then we should keep only words that start with an `m`, have a `p` at a location different from $3$, and don't have `a`, `l` and `e`.
When we have multiple guesses, our list of consistent words should be consistent with all of them, hence as we add more guesses, the list of consistent words will become shorter and shorter. <br>
Run your function on the list of words from (a.), and with the guesses `c("south", "north")` and their corresponding matches: `c(-1, 1, 1, 0, 0)` and `c(0, 1, 0, 0, 0)`. Output the list of consistent words with these two guesses. 

6.a. Consider the following (rather naive) guessing strategy, called **strategy 1:** <br>
- We start with a random word with each letter sampled uniformly and independently from the $26$ English letters. 
- Then, at each turn, we look only at the previous perfect matches (`correct`) to the target word, and ignore matches at the `wrong` location and missing letters. At each place where there is a correct match, we use the correct letter, and at all other locations we keep sampling uniformly from the $26$ letters. We keep going until we get all the five letters correctly (and hence the word). 

We are interested in the number of turns (guesses) needed until we get the correct word. 

Implement a function that receives as input the unknown word, and implements this strategy. The output should be the number of turns it took to guess the word. The function should also record and print guess at each turn, as well as the match array , until the word is guessed correctly.  
Run the function when the unknown word is "mouse", and show the results. 

b. Write a mathematical formula for the distribution of the number of turns needed to guess the target word with this strategy. <br> 
**Hint:** The geometric distribution plays a role here. It is easier to compute the cumulative distribution function.  
Use this formula to compute the expected number of turns for this strategy. <br>
**Note:** The distribution has an infinite support (any positive number of turns has a positive probability), but high number of turns are very rare - you can neglect numbers above $10,000$ when computing the expectation. 

c. Compute empirically the distribution of the number of turns using the following Monte-Carlo simulation:
- Draw $B=100$ random unknown words, uniformly at random from the list of five-letter words in Qu. 5. 
- For each unknown word, run the guessing strategy implemented in (a.) and record the number of turns 
- Compute the average number of turns across all $B=100$ simulations. <br>
Plot the empirical CDF along with the theoretical CDF from (b.) on the same plot. Do they match? 
compare also the empirical expectation with the expectation computed in (b.). How close are they? 


7.a. Implement the following two additional strategies for guessing the word: 

**Strategy 2:** 
- At each stage, we guess the word with the highest likelihood (see Qu. 4.c.), **of the remaining words that are consistent with the previous guesses**. 
- We keep guessing until obtaining the correct word. 

**Strategy 3:** 
The same as strategy 2, but at each stage we guess a random word sampled uniformly from all remaining consistent words (instead of guessing the word with the highest likelihood).

Run both strategies with the unknown word "mouse", and show the guesses and the number of turns for them, in similar to Qu. 6.a.

b. Run $B = 100$ simulations of the games, in similar to Qu. 6.c. 
That is, each time, sample a random unknown word,  run the two strategies $2$ and $3$, and record the number of turns needed to solve `wordle` for both of them. 

- Plot the empirical CDFs of the number of guesses. How similar are they to each other? how similar are they to the CDF of strategy 1? What are the empirical means for both strategies?  


c. (Bonus**) Can you design a better guessing strategy? 
Design and implement a different guessing strategy, run it on $B=100$ random simulations, show the empirical CDF and compute the empirical mean. Your strategy is considered `better` if it shows a significant reduction in the mean number of turns compared to the previous strategies (you should think how to show that the difference is significant)

## 1. 

### a.

```{r}
url <- "https://www.gutenberg.org/files/2701/2701-h/2701-h.htm"
md_html <- read_html(url)
md <- html_text(html_nodes(md_html, "body"))
html_text(html_nodes(md_html,"title"))
```



### b. 

```{r}
per_word_split <- strsplit(md,"\\W+")[[1]]
number_of_words <- length(per_word_split)
len_per_word <- nchar(per_word_split)
barplot(table(len_per_word),col = "dark blue", ylim = c(0,60000), main = "number of words per length ")
median <- median(len_per_word,na.rm = T)
mean <- mean(len_per_word,na.rm = T)
longest <- max(len_per_word)
most_common <- names(sort(table(len_per_word),decreasing = T)[1])
```
There are **`r number_of_words`** words in the textbook 

The median of the length of words is: **`r median`**

the mean of the length of words is: **`r mean`**

the longest of the length of words is: **`r longest`**

the most common of the length of words is: **`r most_common`**

We can see through the bar plot that most of the length of the words are between 2 - 5 while peaking at words of length 3.


### c. 

```{r}
as.data.frame(sort(table(per_word_split),decreasing = TRUE))[1:10,]
```
As we can see the top 10 most frequent words in the book are words like
"the" , "and" , "of" and so on .. 
this make sense and not a surprise seeing how we use these words all the time to build sentences. 



## 2. 

### a. 

Below, we created a list of 137 chapters of the book, including Etymology and Epilogue.

```{r}
chapters = strsplit(md, "\r\n    \r\n    \r\n      \r\n       \r\n    \r\n    \r\n      \r\n    \r\n      |\r\n\r\n      \r\n    \r\n      \r\n       \r\n    \r\n    \r\n      \r\n    \r\n      |\r\n     \r\n    \r\n      \r\n       \r\n    \r\n    \r\n      \r\n    \r\n      ")[[1]]
split_beginning = strsplit(chapters[1], "\r\n      \r\n      \r\n    \r\n      \r\n       \r\n      \r\n        \r\n      \r\n        ")[[1]]
split_ending = strsplit(chapters[137], "\r\n    \r\n\r\n")[[1]]
chapters[1] = split_beginning[2]
chapters[137] = split_ending[1]
```

Now, we used the chapters list we created above, and plotted the number of words per chapter.

```{r}
num_of_words_vec = c()
chapter_index = seq(1:137)
for (chapter_j in chapters) {
  num_words_chapter_j = length(unlist(strsplit(chapter_j,"\\W+")))
  num_of_words_vec = c(num_of_words_vec,num_words_chapter_j)
}
df_chapters_q2a = data.frame(chapter_index, num_of_words_vec)
ggplot(df_chapters_q2a, aes(x = chapter_index, y = num_of_words_vec, ce)) + geom_bar(stat='identity') +scale_x_continuous(breaks =  c(seq(1,137,7))) + ggtitle("Chapters vs Number of words plot") +
  labs(x = "Chapter index", y = "Number of words in the chapter")
```

### b.

We created a function that calculates a vector of relative frequencies of a specific string in each chapter of the book.

```{r}
freq_func_q2 = function(word_q2, chapters_q2) {
  word_porportions_vec = c()
  for (chapter_i in chapters_q2) {
    true_in_i = str_count(chapter_i, as.character(word_q2))
    total_in_i = length(unlist(strsplit(chapter_i,"\\W+")))
    porportions_in_i = true_in_i / total_in_i
    word_porportions_vec = c(word_porportions_vec, porportions_in_i)
  }
  return(word_porportions_vec)
}
```

We calculated the relative frequencies of "Ahab", "Moby" and "sea" strings, and plotted "Ahab" relative frequency.

```{r}
Ahab_q2_b = freq_func_q2("Ahab",chapters)
Moby_q2_b = freq_func_q2("Moby", chapters)
sea_q2_b = freq_func_q2("sea", chapters)
df_words_q2_b = data.frame(chapter_index, Ahab_q2_b, Moby_q2_b, sea_q2_b)

ggplot(df_words_q2_b, aes(x = chapter_index, y = Ahab_q2_b)) + geom_bar(stat='identity') +scale_x_continuous(breaks =  c(seq(1,137,7))) + ggtitle("Porportions of 'Ahab' name apperance vs Chapters") +
  labs(x = "Chapter index", y = "Porportions of 'Ahab' appearnce in the chapter")
```

As above, we plotted "Moby" relative frequency.

```{r}
ggplot(df_words_q2_b, aes(x = chapter_index, y = Moby_q2_b)) + geom_bar(stat='identity') +scale_x_continuous(breaks =  c(seq(1,137,7))) + ggtitle("Porportions of 'Moby' name apperance vs Chapters") +
  labs(x = "Chapter index", y = "Porportions of 'Moby' appearnce in the chapter")
```

And to end question 2, we plotted "sea" relative frequencies.

```{r}
ggplot(df_words_q2_b, aes(x = chapter_index, y = sea_q2_b)) + geom_bar(stat='identity') +scale_x_continuous(breaks =  c(seq(1,137,7))) + ggtitle("Porportions of 'sea' apperance vs Chapters") +
  labs(x = "Chapter index", y = "Porportions of 'sea' appearnce in the chapter")
```

We can see clearly that "sea" appeared many times since the begging, because most of the plot takes place in the sea, while "Ahab" and "Moby" names are relevant to the later parts of the book, as they appear less or not at all in the middle of the book, but alot later in the end.



## 3. 

### a.

i) The exact formula for the probability for Alice and Bob both to 
choose the same word is to calculate all the probabilities of every word 
and square it. meaning that both Alice and Bob fell on the same word.
after that we sum up the squared probabilities and the total sum will be 
our probability for Alice and Bob to fall on the same word.

$p_i$ = the probability for Alice or Bob to fall on word i 
or in other words the relative frequencies of word i in the text. 

$$
	p(same) = \sum_{n=1} ^{N}{p_i^2}
$$

```{r}
all_chapther_together <- c()
 for (i in seq(1:137))
 {all_chapther_together <- paste0(all_chapther_together, chapters[i])}
word_split <- as.data.frame(strsplit(all_chapther_together,"\\W+")[[1]])
colnames(word_split) <- "words"

word_freq <- word_split %>%
  group_by(words)  %>%
   summarise(n = n()) %>% 
  mutate(Freq = n/sum(n))

prob <- sum(word_freq$Freq^2)
```
  
we get that the probability for Alice and Bob to choose the same word is: 
`r prob`



```{r}
set.seed(3)
# 
B <- 100000
count <- 0
for (time in seq(1:B)){
  choices <- sample(word_split$words,2,replace = T)
  if(length(unique(choices)) == 1){count <- count + 1} 
  }
p1 <- count/B
```

ii). On the other hand when we run a simulation for estimating the probability for both of them to choose the same word we get for :

prob = **`r p1``**


We can see that when running a simulation for a hundred thousand times then we get a result that is very alike to our result when deriving it form  an exact probability formula from Q.a .


### b. 

```{r}
unique_words <- unique(word_split$words)
prob_word_i <- 1/length(unique_words)
prob_same_Word <- (prob_word_i^2)*length(unique_words)
# checking which of the probability is bigger from question a or b  
prob_same_Word < prob
```

we can see that the probability in Q.b will lower then in Q.a and this makes sense because when  a word appears in the text more then once it makes the probability for Alice or Bob to choose that word larger. 
and it also make the probability for Alice and bob to choose the same word
larger. 


### 4

## a.

We extracted of the five letter words from the book, and kept only words in which all their letters are in English. we turned all of the words to lower-case. we have 26984 words in total and 2016 unique words.

```{r}
five_letter_word_vec = c()
for (word_q4 in unlist(per_word_split)) {
  if (nchar(word_q4) == 5 & grepl("[A-Za-z]", word_q4) & !grepl("[[:digit:]]", word_q4)) {
      five_letter_word_vec = c(five_letter_word_vec, as.character(word_q4))
  }
}
five_letter_word_vec = str_to_lower(five_letter_word_vec)
five_letter_word_vec = five_letter_word_vec[!five_letter_word_vec %in% c("c\346sar","vers\342")]
unique_five_letter_vec = unique(five_letter_word_vec)
```

Below are the 10 most frequent five letter words and their frequency.

```{r}
word_freq_df = as.data.frame(word_freq)
five_letter_freq = word_freq_df[word_freq_df$words %in% unique_five_letter_vec,]
five_letter_freq = five_letter_freq[order(five_letter_freq$Freq, decreasing = TRUE),]
five_letter_freq[c(1:10),]
```

## b

We created letter frequency for each of the five potential positions in a five letter word. meaning we created a 26 by 5 table, rows are lower-case letters and columns are 1 to 5 indexes in a five letter word. we calculated the relative frequency of each letter in each index. each letter has 5 different relative frequency to all the 26 possible letters, in each of the indexes.

```{r}
five_letter_heatmap_df = data.frame(matrix(0,26,5))
sum = 0
rownames(five_letter_heatmap_df) = str_to_lower(LETTERS)
colnames(five_letter_heatmap_df) = c(1:5)
for (word_q4_b in five_letter_word_vec) {
  splitted_word = strsplit(word_q4_b, split = "")
  splitted_letter_index = 0
  for (splitted_letter in unlist(splitted_word)) {
    sum = sum + 1
    splitted_letter_index = splitted_letter_index + 1
    five_letter_heatmap_df[as.character(splitted_letter),splitted_letter_index] =
      five_letter_heatmap_df[as.character(splitted_letter),splitted_letter_index] + 1
  }
}
five_letter_heatmap_df$`1` = five_letter_heatmap_df$`1` / sum(five_letter_heatmap_df$`1`)
five_letter_heatmap_df$`2` = five_letter_heatmap_df$`2` / sum(five_letter_heatmap_df$`2`)
five_letter_heatmap_df$`3` = five_letter_heatmap_df$`3` / sum(five_letter_heatmap_df$`3`)
five_letter_heatmap_df$`4` = five_letter_heatmap_df$`4` / sum(five_letter_heatmap_df$`4`)
five_letter_heatmap_df$`5` = five_letter_heatmap_df$`5` / sum(five_letter_heatmap_df$`5`)
```

Using the table we created in the beginning of 4b, we presented a heat map table, showing the most frequent letter in each of the 5 indexes.

The most frequent letter in each index:
Index 1 = w
Index 2 = h
Index 3 = a
Index 4 = e
Index 5 = e

We can see that from index 3 to 5 vowel letters are more frequent.

```{r}
five_letter_heatmap_matrix = data.matrix(five_letter_heatmap_df)
five_letter_heatma = heatmap(five_letter_heatmap_matrix, Rowv = NA, Colv = NA, value, scale = "row", main = "Q4 Characters frequency heatmap", xlab = "Letter's position", ylab = "Letter", cexRow = 0.55, cexCol = 1.25)
legend(x="right", legend=c(1:10),fill=heat.colors(10))
```

## C

We used the table of probabilites we created in 4b, and created a function takes a table of probabilites and a array of strings, and calculated each string its probability, by multiplying each letter's probability with all the other letter's probabilities to create the probability of the specific word.

```{r}
likelihood_of_string = function(table_of_prob, string_array){
  prob_vector_likelihood = c()
  for (word_q4_c in string_array) {
    letter_index_q4_c = 0
    total_word_prob = 1
    word_q4_c_letters = strsplit(word_q4_c, split = "")
    for (letter_q4_c in unlist(word_q4_c_letters)) {
      letter_index_q4_c = letter_index_q4_c + 1
      total_word_prob = total_word_prob * table_of_prob[letter_q4_c, letter_index_q4_c]
    }
    prob_vector_likelihood = c(prob_vector_likelihood, total_word_prob)
  }
  return(prob_vector_likelihood)
}
```

We used the function above to compute the probability of each five letter word, and we presented the top 10 words with highest probabilty.

```{r}
options(scipen=999)
prob_vec_q4_c = likelihood_of_string(five_letter_heatmap_df, unique_five_letter_vec)
prob_df_q4_c = as.data.frame(cbind(unique_five_letter_vec, prob_vec_q4_c))
prob_df_q4_c = prob_df_q4_c[order(prob_df_q4_c$prob_vec_q4_c, decreasing = TRUE),]
colnames(prob_df_q4_c) = c("Unique five letter words", "Probabillity")
top_n(prob_df_q4_c, 10)
```

### 5.

## a.

Below is the "wordle_match" function we got from our instructors. the function creates a vector of "correct" and "wrong" position for each guessed letter in a guess word.

```{r, cache=TRUE}
# Helper function: 
wordle_match <- function(guess, word)  # 1: correct location, -1: wrong location, 0: missing
{
  L <- nchar(guess)
  match <- rep(0, L)
  for(i in 1:L)
  {
    if(grepl(substr(guess, i, i), word, fixed=TRUE))
      {match[i] = -1}
    if(substr(guess, i, i) == substr(word, i, i))
    {      match[i] = 1}
  }
  
  return(match)
}

```

As in 4b, we created a 26 by 5 table of probabilities for each word in each index, for five letter words we got from our instructors.
We can see differences between 4b table and this table, vowel letters are more frequent here, probably because it wasn't taken from a book which has a very specific set of words to describe the plot (whale was the most frequent word)

```{r}
Question5_words = read.csv("Question5.csv")
colnames(Question5_words) = "FiveLetterWords"
Question5_words$FiveLetterWords = str_to_lower(Question5_words$FiveLetterWords)
Question5_words_vec = Question5_words$FiveLetterWords

Question5_wordsletters = data.frame(matrix(0,26,5))
sum = 0
rownames(Question5_wordsletters) = str_to_lower(LETTERS)
colnames(Question5_wordsletters) = c(1:5)
for (word_q5 in Question5_words_vec) {
  splitted_word = strsplit(word_q5, split = "")
  splitted_letter_index = 0
  for (splitted_letter in unlist(splitted_word)) {
    sum = sum + 1
    splitted_letter_index = splitted_letter_index + 1
    Question5_wordsletters[as.character(splitted_letter),splitted_letter_index] =
      Question5_wordsletters[as.character(splitted_letter),splitted_letter_index] + 1
  }
}
Question5_wordsletters$`1` = Question5_wordsletters$`1` / sum(Question5_wordsletters$`1`)
Question5_wordsletters$`2` = Question5_wordsletters$`2` / sum(Question5_wordsletters$`2`)
Question5_wordsletters$`3` = Question5_wordsletters$`3` / sum(Question5_wordsletters$`3`)
Question5_wordsletters$`4` = Question5_wordsletters$`4` / sum(Question5_wordsletters$`4`)
Question5_wordsletters$`5` = Question5_wordsletters$`5` / sum(Question5_wordsletters$`5`)
```

## b.

Now, we created a function that recives an array of guess words, an array of their matching vectors, and a dictionary.
In the function, we created a 5 column data frame, for each of the 5 possible indexes in a five letter word. we splitted each word to five letters, and inserted each letter to the relevant index in the data frame (each row of the data frame is a word). Than, we removed each word that has a 0 letter in it, and a word that doesn't have a 1 letter in the right position. than we removed all words with -1 letters in the specific column which was -1 for the letter, and removed all words which doesn't have the -1 letter in them, just not in the specific index.

```{r}
Q5_wordle = function(guess_words, match_to_unk, Question5_words_vec){
  Q5_b_df = as.data.frame(matrix("", nrow = length(Question5_words_vec), ncol = 5))
rownames(Q5_b_df) = Question5_words_vec
colnames(Q5_b_df) = c(1:5)
for (word in Question5_words_vec) {
  splitted_word = unlist(strsplit(word,""))
  for (i in 1:5) {
    Q5_b_df[as.character(word),i] = splitted_word[i]
  }
}
  match = match_to_unk
  guess_words = unlist(guess_words)
  i = 0
  for (guess in guess_words){
    i = i + 1
    splitted_guess = unlist(strsplit(
      guess,""))
  for (j in 1:5) {
    if (match[[i]][j] == 1){
      Q5_b_df = Q5_b_df %>% filter(Q5_b_df[j] == splitted_guess[j])
    }
    if (match[[i]][j] == 0){
      for (k in 1:5) {
      Q5_b_df = Q5_b_df %>% filter(Q5_b_df[k] != splitted_guess[j])
      }
    }
    if (match[[i]][j] == -1){
      Q5_b_df = Q5_b_df %>% filter(Q5_b_df[j] != splitted_guess[j])
      Q5_b_df <- Q5_b_df[apply(Q5_b_df,1,function(x){any(x==splitted_guess[j])}),]
    }
  }
  }
  return(rownames(Q5_b_df))
}
```


### 6

## a.

we built a function that will calculate with the naive strategy of guessing letters randomly from a uniform distribution until guessing the right letter.
when the letters are guess correctly to the unknown word we continue to sample 
letters for all the letters not yet guess correctly until there are no more letters to correct .


```{r,}
naive_strategy <- function(unknow_word,print = T){
  j <- 0
  count <- 0
  index_na <- seq(1:5)
  vec <- rep(1,5)
  
  while(j != 5){
  guess <- sample(x = letters,size = 5-j, replace = T)
  guess <- replace(vec, list = index_na, values = guess)
  guess_1 <- paste0(guess,collapse = "")
  match <- paste0(wordle_match(guess_1,unknow_word),collapse = ",")
  vec <- ifelse(guess == unlist(str_split(unknow_word, "")), guess, NA)
  count <- count + 1
  v <- c(count,guess_1, match)
  if(print == T){
  print(c("round" , "guess", "   match"))
  print(c( count , guess_1, match))
  }
  index_na <- which(is.na(vec))
  j <-  5-length(index_na)
  }
  return(count)
}

naive_strategy("mouse")
```


## b. 

let X represent the number of turns needed to guess the target word with
this naive strategy.
and let $X_i$ represent the number of turns needed to guess letter i 
thus, $X = \sum_{i=1} ^{5}{x_i}$

the probability that it will take exactly X times Without loss of generality
we will use $x_1$ is:

$$P(x=X) = [P(x\leq X+1)]^5 - [P(x_1\leq X)]^5$$


the cumulative distribution function of X is :


$$ P(x \leq X) = 1 - (1-p)^{x - 1} $$



$$P(x=X) = [P(x\leq X+1)]^5 - [P(x_1\leq X)]^5$$

$$P(X=x) =  [1 - (1-p)^{x_1}]^5 - [1 -  (1-p)^{x_1 - 1}]^5$$

where : $p = \frac{1}{26}$ the probability of correctly guessing letter $x_i$.

because each $x_i$ is i.i.d we can multiple probabilities as displayed above .

To compute the expected value of the distribution we will multiply every value

between [0 - 10,000] with the probability $P(X= x_i) \quad \ \forall  \quad 1\leq i \leq 10,000$  


```{r}
prob <- function(x)((1-(25/26)^x)^5 - (1 - (25/26)^(x-1))^5)
N <- 10000
expected_value <- rep(NA,N)
for(x in seq(N)){
  expected_value[x] <- x*prob(x)
  }
expected <- sum(expected_value)
expected
```

We can see that the  expected number of turns for this strategy using the 
 mathematical formula is **`r expected`**


### c. 

```{r}
B <- 100 
random_words <- Question5_words$FiveLetterWords
samp <- sample(random_words,100,replace = T)
record_turns <- rep(NA,100)
for(i in seq(1:100)){
  record_turns[i] <- naive_strategy(samp[i],F)
}
mean_of_samp <- mean(record_turns)
cat(paste0(" the empirical expectation is ", mean_of_samp, " while the expectaion computed is ", expected))

# calculating a vector of probability per x for the mathematical formula developed above. 
pr <- c()
for(i in seq(200)){
  pr[i] <- prob(i)
}
# calculating ECDF for the vector of probability per x 
ecdf <- cumsum(pr)

plot(ecdf(record_turns), col = "blue",main="ECDF vs TCDF", xlab="X")
lines(seq(1:200),ecdf,col="red",lwd = 3)
legend(140,0.6, legend=c("TCDF", "ECDF"),
       col=c("red", "blue"), lty=1:2, cex=1)
```

We can see that both the ECDF and TCDF are very alike and seem to have very similar distributions .


### 7 

## a

We created the strategy 2 function below. We built a data frame with the words and their probabilities (using likelihood function). Now, each time we guessed the word with the highest probability, found out what was it's match with the unknown word, and used the Q5_wordle function to save only the words that fit the match. we kept on doing so until we found the unknown word, that was in the dictionary.

**Strategy 2**
```{r}
strategy_2 = function(unknown_word,print = T){
  prob_vec_q7 = likelihood_of_string(Question5_wordsletters, Question5_words_vec)
Question7_words_prob_df = as.data.frame(cbind(Question5_words_vec, prob_vec_q7))
colnames(Question7_words_prob_df) = c("Words", "Prob")
Question7_words_prob_df = Question7_words_prob_df[order(Question7_words_prob_df$Prob, decreasing = TRUE),]
uptodate_df = Question7_words_prob_df
  count = 0
  guess_words_vector = c()
  strategy2_function = function(uptodate_df){
    max_prob_uptodate = uptodate_df$Words[which(uptodate_df$Prob == max(uptodate_df$Prob))]
    match_unknown_word = as.vector(wordle_match(max_prob_uptodate, unknown_word))
    relevant_words = Q5_wordle(max_prob_uptodate, list(match_unknown_word), uptodate_df$Words)
    uptodate_df = uptodate_df %>% filter(uptodate_df$Words %in% relevant_words)
    return(uptodate_df)
  }
  while (length(uptodate_df$Words) > 1) {
    count = count + 1
    max_prob_uptodate = uptodate_df$Words[which(uptodate_df$Prob == max(uptodate_df$Prob))]
    if(print == T){
    print("turn  guess")
    print(c(count, max_prob_uptodate))
    }
    uptodate_df = strategy2_function(uptodate_df)
  }
  return(count)
}
strategy_2("mouse")

```

**Strategy 3**
```{r}
dict_word <- Question5_words$FiveLetterWords
Strategy_3 <- function(word,print = T){
  guess <- ""
  count <- 0
  while(word != guess){
    count <- count + 1
    guess <- sample(dict_word,1)
    if(print == T){
    print("turn  guess ")
    print(c( count,   guess))
    }
    match <- list(wordle_match(guess,word))
    dict_word <- Q5_wordle(guess, match , dict_word)
  }
  return(count)
}
Strategy_3("mouse")
```

## b.

```{r}
B = 100 
strategy2 <- rep(NA,B)
strategy3 <- rep(NA,B)
for(i in seq(B)){
  sample_word <- sample(dict_word,size = 2,replace = T)
  strategy2[i] <- strategy_2(sample_word[1],F)
  strategy3[i] <- Strategy_3(sample_word[2],F)
  }

mean_of_2 <- mean(strategy2)
mean_of_3 <- mean(strategy3)
cat(paste0(" the empirical mean of strategy 2 is ", mean_of_2, " while the empirical mean of strategy 3 is is ", mean_of_3))


plot(ecdf(record_turns), col = "blue",main="Strategy 1 ECDF", xlab="X")
par(mfrow = c(1,2))
plot(ecdf(strategy2), col = "Red",main="Strategy 2 ECDF", xlab="X")
plot(ecdf(strategy3), col = "Green",main="Strategy 3 ECDF", xlab="X")
```

We can see that strategy 2 and 3 need much less tries then strategy 1 for 
for guessing the correct word . 

We can see that strategy 2 and 3 look alike when it comes to number of tries on average and all together the distributions of strategy 2 and 3 look alike. 